{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Stage 5: Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Steps Involved in Evaluating and Validating Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Set Up Evaluation Metrics\n",
    "2. Interpret Training Loss Curve\n",
    "3. Run Validation Loops\n",
    "4. Monitor and Interpret Results\n",
    "5. Hyperparameter Tuning and Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Setting Up Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Cross-entropy is a key metric for evaluating LLMs during training or fine-tuning. Originating from\n",
    " information theory, it quantifies the difference between two probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Importance of Cross-Entropy for LLM Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Cross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model to produce high-quality predictions by minimising discrepancies between the predicted and actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Beyond Cross-Entropy: Advanced LLM Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Perplexity: Perplexity measures how well a probability distribution or model predicts a sample. In the context of LLMs, it evaluates the modelâ€™s uncertainty about the next word in a sequence. Lower perplexity indicates better performance, as the model is more confident in its predictions.\n",
    "\n",
    "+ Factuality: Factuality assesses the accuracy of the information produced by the LLM. It is particularly important for applications where misinformation could have serious consequences. Higher factuality scores correlate with higher output quality.\n",
    "\n",
    "+ LLM Uncertainty: LLM uncertainty is measured using log probability, helping to identify low-quality generations. Lower uncertainty indicates higher output quality. \n",
    "\n",
    "+ Prompt Perplexity: This metric evaluates how well the model understands the input prompt. \n",
    "\n",
    "+ Context Relevance: In retrieval-augmented generation (RAG) systems, context relevance measures how pertinent the retrieved context is to the user query. \n",
    "\n",
    "+ Completeness\n",
    "\n",
    "+ Chunk Attribution and Utilisation: These metrics evaluate how effectively the retrieved chunks of information contribute to the final response.\n",
    "\n",
    "+ Data Error Potential \n",
    "\n",
    "+ Safety Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Understanding the Training Loss Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " The training loss curve plots the loss value against training epochs and is essential for monitoring model\n",
    " performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Interpreting Loss Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " An ideal training loss curve shows a rapid decrease in loss during initial stages, followed by a gradual\n",
    " decline and eventual plateau. Specific patterns to look for include:\n",
    " 1. Underfitting: High loss value that does not decrease significantly over time, suggesting the model\n",
    " cannot learn the data.\n",
    " 2. Overfitting: Decreasing training loss with increasing validation loss, indicating the model mem\n",
    "orises the training data.\n",
    " 3. Fluctuations: Significant variations may indicate a high learning rate or noisy gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Avoiding Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Techniques to prevent overfitting include:\n",
    " 1. Regularisation: Adds a penalty term to the loss function to encourage smaller weights.\n",
    " 2. Early Stopping: Stops training when validation performance no longer improves.\n",
    " 3. Dropout: Randomly deactivates neurons during training to reduce sensitivity to noise.\n",
    " 4. Cross-Validation: Splits data into multiple subsets for training and validation to assess model\n",
    " generalisation.\n",
    " 5. Batch Normalisation: Normalises inputs to each layer during training to stabilise the learning\n",
    " process.\n",
    " 6. Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount of diverse\n",
    " data and batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Sources of Noisy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " 1. Learning Rate Scheduling: Gradually decreasing the learning rate during training can reduce\n",
    " the impact of noisy gradients.\n",
    " 2. Gradient Clipping: Setting a threshold for gradient values prevents large updates that can\n",
    " destabilise training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running Validation Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " 1. Split Data: Divide the dataset into training and validation sets.\n",
    " 2. Initialise Validation: Evaluate the model on the validation set at the end of each epoch.\n",
    " 3. Calculate Metrics: Compute relevant performance metrics, such as cross-entropy loss.\n",
    " 4. Record Results: Log validation metrics for each epoch.\n",
    " 5. Early Stopping: Optionally stop training if validation loss does not improve for a predefined\n",
    " number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Monitoring and Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " 1. Consistent Improvement: Indicates good model generalisation if both training and validation\n",
    " metrics improve and plateau.\n",
    " 2. Divergence: Suggests overfitting if training metrics improve while validation metrics deteriorate.\n",
    " 3. Stability: Ensure validation metrics do not fluctuate significantly, indicating stable training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Hyperparameter Tuning and Other Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " 1. Learning Rate: Determines the step size for updating model weights. A good starting point is\n",
    " 2e-4, but this can vary.\n",
    " 2. Batch Size: Larger batch sizes lead to more stable updates but require more memory.\n",
    " 3. Number of Training Epochs: Balancing the number of epochs ensures the model learns suffi\n",
    "ciently without overfitting or underfitting.\n",
    " 4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
