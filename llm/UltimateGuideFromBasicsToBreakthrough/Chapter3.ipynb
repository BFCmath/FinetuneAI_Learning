{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Chapter 3: Stage 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Steps Involved in Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The first step in data preparation is to collect data from various sources. These sources can be in any\n",
    " format such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to\n",
    " gather the data efficiently and accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:700px\">\n",
    "    <img src=\"image/data_collect_library.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Data Preprocessing and Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Data preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step\n",
    " involves tasks such as cleaning the data, handling missing values, and formatting the data to match the\n",
    " specific requirements of the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:300px\">\n",
    "    <img src=\"image/data_preprocess_library.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Handling Data Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Over-sampling and Under-sampling**: \n",
    "+ Techniques like SMOTE (Synthetic Minority Over\n",
    "sampling Technique) generate synthetic examples to achieve balance.\n",
    "+ Python Library: imbalanced-learn\n",
    "+ Description: imbalanced-learn provides various methods to deal with imbalanced datasets, in\n",
    "cluding oversampling techniques like SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Adjusting Loss Function**: Modify the loss function to give more weight to the minority class,\n",
    " setting class weights inversely proportional to the class frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Focal Loss**: A variant of cross-entropy loss that adds a factor to down-weight easy examples and\n",
    " focus training on hard negatives.\n",
    "+ **Python Library**: focal loss\n",
    "+ **Description**: The focal loss package provides robust implementations of various focal loss func\n",
    "tions, including BinaryFocalLoss and SparseCategoricalFocalLoss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Cost-sensitive Learning**: Incorporating the cost of misclassifications directly into the learning\n",
    " algorithm, assigning a higher cost to misclassifying minority class samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Ensemble Methods**: Using techniques like bagging and boosting to combine multiple models\n",
    " and handle class imbalance.\n",
    "+ Python Library: sklearn.ensemble\n",
    "+ Description: scikit-learn provides robust implementations of various ensemble methods, including\n",
    " bagging and boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**StratifiedSampling**: Ensuring that each mini-batch during training contains an equal or proportional representation of each `class.\n",
    "+ PythonLibrary: sklearn.model selection.StratifiedShuffleSplit\n",
    "+ Description: scikit-learn offers tools for stratifiedsampling, ensuring balanced representation\n",
    " across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Data Cleaning**: Removing noisy and mislabelled data, which can disproportionately affect the minority class.\n",
    "+ Python Library: pandas.DataFrame.sample\n",
    "+ Description: pandas provides methods for sampling data from DataFrames, useful for data cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Using Appropriate Metrics**: Metrics like Precision-Recall AUC, F1-score, and Cohen’s Kappa are more informative than accuracy when dealing with imbalanced datasets.\n",
    "+ Python Library: sklearn.metrics\n",
    "+ Description: scikit-learn offers a comprehensive set of tools for evaluating the performance of classification models, particularly with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Splitting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Splitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using an 80:20 ratio. Different techniques include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Random Sampling: Selecting a subset of data randomly to create a representative sample\n",
    "+ Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain class balance.\n",
    "+  K-Fold Cross Validation: Splitting the dataset into K folds and performing training and validation K times.\n",
    "+  Leave-One-Out Cross Validation: Using a single data point as the validation set and the rest for training, repeated for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Existing and Potential Research Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Data annotation involves labelling or tagging textual data with specific attributes relevant to the model’s training objectives. This process is crucial for supervised learning tasks and greatly influences the performance of the fine-tuned model. Recent research highlights various approaches to data annotation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Human Annotation\n",
    "+ Semi-automatic Annotation\n",
    "+ Automatic Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Data Augmentation (DA) techniques expand training datasets artificially to address data scarcity and improve model performance. Advanced techniques often used in NLP include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Word Embeddings\n",
    "+ Back Translation\n",
    "+ Adversarial Attacks\n",
    "+ NLP-AUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Synthetic Data Generation using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Large Language Models (LLMs) can generate synthetic data through innovative techniques such as:\n",
    "+ Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant and high-quality synthetic data\n",
    "+ Multi-Step Generation: Employing iterative generation processes where LLMs generate initial data that is refined through subsequent steps. This method can produce high-quality synthetic data for various tasks, including summarising and bias detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Challenges in Data Preparation for Fine-Tuning LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Available LLM Fine-Tuning Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Best Practices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
