{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Chapter 4: Stage 2 - Model Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Steps Involved in Model Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:700px\">\n",
    "    <img src=\"image/setupllm.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. **Set Up the Environment**: Configure your environment, such as setting up GPU/TPU usage if available, which can significantly speed up model loading and inference.\n",
    "2. **Install the Dependencies**: Ensure that all necessary software and libraries are installed. This typically includes package managers like pip and frameworks like PyTorch or TensorFlow.\n",
    "3. **Import the Libraries**: Import the required libraries in your script or notebook. Common libraries include transformers from Hugging Face, torch for PyTorch, and other utility libraries.\n",
    "4. **Choose the Language Model**: Select the appropriate pre-trained language model based on your task requirements. This could be models like BERT, GPT-3, or others available on platforms like Hugging Face’s Model Hub.\n",
    "5. **Download the Model from the Repository**: Use the chosen framework’s functions to download the pre-trained model from an online repository. For instance, using transformers, you might use AutoModel.from_pretrained(’model_name’).\n",
    "6. **Load the Model in the Memory**: Load the model into memory, ready for inference or further fine-tuning. This step ensures the model weights are initialised and ready for use.\n",
    "7. **Execute Tasks**: Perform the desired tasks using the loaded model. This could involve making predictions, generating text, or fine-tuning the model on a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Tools and Libraries for Model Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Python Library: **HuggingFace**\n",
    "\n",
    "Description: HuggingFace is renowned for its support of numerous pre-trained large language models, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace, enables users to access these models via classes such as AutoModelForCausalLM. This library supports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers library includes the ”pipeline” feature, making it easy to use pre-trained models for various tasks.\n",
    "\n",
    "2. Python Framework: **PyTorch**\n",
    "\n",
    "Description: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning large language models. It provides a flexible and efficient platform for building and deploying deep learning models. HuggingFace’s transformers library bridges the gap between PyTorch and other frameworks, enhancing its usability for state-of-the-art language models.\n",
    "\n",
    "3. Python Framework: **TensorFlow**\n",
    "\n",
    "Description: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning large language models. Similar to PyTorch, it benefits from the HuggingFace transformers library, which provides a versatile and user-friendly API and interface for working with the latest advancements in large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Challenges in Model Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:700px\">\n",
    "    <img src=\"image/challenge_init.png\" alt=\"\" />\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
