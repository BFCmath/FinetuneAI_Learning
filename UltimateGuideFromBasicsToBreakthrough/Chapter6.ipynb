{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Chapter 6: Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Steps Involved in Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. **Initialise the Pre-Trained Tokenizer and Model**\n",
    "2. **Modify the Model’s Output Layer**\n",
    "3. **Choose an Appropriate Fine-Tuning Strategy**: Select the fine-tuning strategy that best fits the task and the model architecture. Some Options include:\n",
    "+ Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classification, and question answering, adapt the model using relevant datasets.\n",
    "+ Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant to specific domains, such as medical, financial, or legal fields.\n",
    "+ Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters allow for fine-tuning with reduced computational costs by updating a small subset of model parameters.\n",
    "+ Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning new tasks by updating only half of the model’s parameters during each fine-tuning round.\n",
    "\n",
    "4. **Set Up the Training Loop**\n",
    "5. **Incorporate Techniques for Handling Multiple Tasks**\n",
    "6. **Monitor Performance on a Validation Set**\n",
    "7. **Optimise Model Using Advanced Techniques**: Employ techniques such as Proximal Policy Optimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO) for aligning model outputs with human preferences. These techniques are particularly useful in fine-tuning models for tasks requiring nuanced decision-making or human-like responses.\n",
    "\n",
    "8. **Prune and optimise the Model** (if necessary)\n",
    "9. **Continuous Evaluation and Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Fine-Tuning Strategies for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Task-Specific Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:400px\">\n",
    "    <img src=\"image/task_specific.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Domain-Specific Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Parameter-Efficient Fine-Tuning (PEFT) Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Parameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained language models to various applications with remarkable efficiency. PEFT methods fine-tune only a small subset of (additional) model parameters while keeping most of the pre-trained LLM parameters frozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue of catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and experience a significant performance decline on previously learned tasks when trained on new datasets. PEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in low-data scenarios, and exhibit better generalisation to out-of-domain contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Adapter-based methods introduce additional trainable parameters after the attention and fully connected\n",
    " layers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. \n",
    " \n",
    " The specific approach varies depending on the adapter; it might involve adding an extra layer or representing the\n",
    " weight updates delta as a low-rank decomposition of the weight matrix.\n",
    " \n",
    "  Regardless of the method,\n",
    " adapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for\n",
    " the training of larger models with fewer resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:600px\">\n",
    "    <img src=\"image/peft.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Low-Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Low-Rank Adaptation (LoRA) is a technique designed for fine-tuning large language models, which\n",
    " modifies the fine-tuning process by freezing the original model weights and applying changes to a separate\n",
    " set of weights, added to the original parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " LoRA transforms the model parameters into a lower\n",
    "rank dimension, reducing the number of trainable parameters, speeding up the process, and lowering\n",
    " costs.\n",
    " \n",
    "  This method is particularly useful in scenarios where multiple clients require fine-tuned models\n",
    " for different applications, allowing for the creation of specific weights for each use case without the\n",
    " need for separate models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:500px\">\n",
    "    <img src=\"image/lora.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:300px\">\n",
    "    <img src=\"image/lora_weight.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " QLoRA is an extended version of LoRA designed for greater memory efficiency in large language mod\n",
    "els (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored\n",
    " in a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint.\n",
    " This allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the\n",
    " weights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements. Despite the reduction in bit precision, QLoRA maintains performance levels comparable\n",
    " to traditional 16-bit fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Weight-Decomposed Low-Rank Adaptation (DoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Weight-Decomposed Low-Rank Adaptation (DoRA) is a novel fine-tuning methodology designed to\n",
    " optimise pre-trained models by decomposing their weights into magnitude and directional components.\n",
    "\n",
    " This approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facili\n",
    "tating substantial parameter updates without altering the entire model architecture. \n",
    "\n",
    "DoRA addresses the computational challenges associated with traditional full fine-tuning (FT) by maintaining model\n",
    " simplicity and inference efficiency, while simultaneously bridging the performance gap typically observed\n",
    " between LoRA and FT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:600px\">\n",
    "    <img src=\"image/dora.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Comparison between LoRA and DoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:300px\">\n",
    "    <img src=\"image/lora_dora.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Fine-Tuning with Multiple Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The PEFT library simplifies the process of merging adapters with its add_weighted_adapter function 3, which offers three distinct methods:\n",
    "\n",
    "1. Concatenation: This straightforward method concatenates the parameters of the adapters. For instance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This method is highly efficient.\n",
    "2. Linear Combination: Although less documented, this method appears to perform a weighted sum of the adapters’ parameters.\n",
    "3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While versatile, it is notably slower than the other methods, particularly for adapters with high ranks (greater than 100), which can take several hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:600px\">\n",
    "    <img src=\"image/multiple_adapter.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Half Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Half Fine-Tuning (HFT) is a technique designed to balance the retention of foundational knowledge\n",
    " with the acquisition of new skills in large language models (LLMs).\n",
    " \n",
    "HFT involves freezing half of the\n",
    " model’s parameters during each fine-tuning round while updating the other half, allowing the model to\n",
    " retain pre-trained knowledge and enhance new task performance without altering the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Benefits of using Half Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Recovery of Pre-Trained Knowledge\n",
    "\n",
    "2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses the performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in balancing knowledge retention with task-specific learning.\n",
    "\n",
    "3. Robustness\n",
    "\n",
    "4. Simplicity and Scalability\n",
    "\n",
    "5. Versatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:500px\">\n",
    "    <img src=\"image/hft.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Comparison between HFT and LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:600px\">\n",
    "    <img src=\"image/hft_vs_lora.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Lamini memory tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Foundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes\n",
    " training for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion\n",
    " tokens.\n",
    " \n",
    "  This approach results in substantial loss and is geared more towards enhancing generalisation\n",
    " and creativity where a degree of randomness in token selection is permissible. \n",
    " \n",
    " However, it falls short for\n",
    " tasks demanding high factual precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In contrast, Lamini Memory Tuning delves deeper by analysing\n",
    " the loss of individual facts, significantly improving the accuracy of factual recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "By augmenting a\n",
    " model with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B\n",
    " parameters for weights), Lamini enables the model to memorise and accurately recall a significant number\n",
    " of facts, closely aligning performance with LLM scaling laws without compromising on generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Lamini-1- A model architecture based on Lamini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Departing from traditional transformer-based designs, the Lamini-1 model architectur employs a massive mixture of memory experts (MoME). This system features a pre-trained transformer\n",
    " backbone augmented by adapters that are dynamically selected from an index using cross-attention\n",
    " mechanisms. \n",
    " \n",
    " These adapters function similarly to experts in MoE (Mixture of Expert) architectures, and the network is\n",
    " trained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\n",
    " in the selected experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:450px\">\n",
    "    <img src=\"image/lamini-1.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Systems Optimisations for Banishing Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " The MoME architecture is designed to minimise the computational demand required to memorise facts.\n",
    " During training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\n",
    " the backbone network and the cross attention used to select the expert are frozen, and gradient descent\n",
    " steps are taken until the loss is sufficiently reduced to memorise the fact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Mixture of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " A mixture of experts (MoE) is an architectural design for neural networks that divides the computation\n",
    " of a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnet\n",
    "works, referred to as ”experts”.\n",
    "\n",
    " Each expert independently carries out its computation, and the results\n",
    " are aggregated to produce the final output of the MoE layer.\n",
    " \n",
    "  MoE architectures can be categorised as\n",
    " either dense, where every expert is engaged for each input, or sparse, where only a subset of experts is\n",
    " utilised for each input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Mixtral 8x7B Architecture and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Mixtral 8x7B employs a Sparse Mixture of Experts (SMoE) architecture, mirroring the\n",
    " structure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer.\n",
    " \n",
    "  For every\n",
    " token at each layer, a router network selects two experts to process the current state and combine their\n",
    " outputs. Although each token interacts with only two experts at a time, the selected experts can vary at\n",
    " each timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion\n",
    " active parameters during inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:300px\">\n",
    "    <img src=\"image/mistral.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Mixture of Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "A recent study has investigated\n",
    " leveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a\n",
    " method known as Mixture of Agents (MoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:500px\">\n",
    "    <img src=\"image/moa.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To enhance collaboration among multiple LLMs, it is essential to understand their individual strengths and classify them accordingly. The classification includes:\n",
    "\n",
    "1. Proposers: These models excel at generating valuable reference responses for other models. While they may not perform exceptionally on their own, they provide useful context and varied perspectives that improve the final output when utilised by an aggregator.\n",
    "2. Aggregators: These models are adept at merging responses from various models into a single high-quality result. An effective aggregator should maintain or even enhance the quality of the final response, regardless of the quality of the individual inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Proximal Policy Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Direct Preference Optimisation (DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Optimised Routing and Pruning Operations "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
