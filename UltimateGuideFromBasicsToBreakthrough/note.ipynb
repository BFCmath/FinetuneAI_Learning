{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Source**: [original paper](https://arxiv.org/pdf/2408.13296v1) and many online sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Note**: This is a summary of what I've learned and understood from the original papers. I've also included additional information collected from online sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Chaper 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Background of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Large Language Models (LLMs) represent a significant leap in computational systems capable of under\n",
    "standing and generating human language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Notable examples, such as GPT-3 and GPT-4, leverage the self-attention mecha\n",
    "nism within Transformer architectures to efficiently manage sequential data and understand long-range\n",
    " dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Historical Development and Key Milestones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Language models are fundamental to natural language processing (NLP), leveraging mathematical tech\n",
    "niques to generalise linguistic rules and knowledge for tasks involving prediction and generation.\n",
    "\n",
    " Over\n",
    " several decades, language modelling has evolved from early statistical language models (SLMs) to to\n",
    "day’s advanced large language models (LLMs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:400px\">\n",
    "    <img src=\"image/timeline.png\" alt=\"timeline\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Evolution from Traditional NLP Models to State-of-the-Art LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Statistical Language Model (SLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Emerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\n",
    " likelihood of sentences within texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Probability: SLMs assign probabilities to sequences of words or sentences.\n",
    "+ N-gram models: The most common type, especially for earlier SLMs. They predict the next word based on the previous n-1 words.\n",
    "+ Limitations: Traditional SLMs struggle with long-range dependencies and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Neural Language Model (NLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "NLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors\n",
    " enable computers to understand word meanings. \n",
    " \n",
    " Tools like Word2Vec represent words in a vector\n",
    " space where semantic relationships are reflected in vector angles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The input layer concatenates word vectors,\n",
    " the hidden layer applies a non-linear activation function, and the output layer predicts subsequent words\n",
    " using the Softmax function to transform values into a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Pretrained Language Model (PLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " PLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language\n",
    " structures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This ”pre-training\n",
    " and fine-tuning” paradigm, exemplified by GPT-2 and BERT, has led to diverse and effective model\n",
    " architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Large Language Models (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " LLMs like GPT-3, GPT-4, PaLM, and LLaMA are trained on massive text corpora with tens of\n",
    " billions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed by alignment with human values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Overview of Current Leading LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "LLMs’ rapid development has spurred research into architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and integrating multi-modal data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:500px\">\n",
    "    <img src=\"image/LLMdimension.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Types of LLM Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Unsupervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " This method does not require labelled data. Instead, the LLM is exposed to a large corpus of unla\n",
    "belled text from the target domain, refining its understanding of language. This approach is useful for\n",
    " new domains like legal or medical fields but is less precise for specific tasks such as classification or\n",
    " summarisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "SFT involves providing the LLM with labelled data tailored to the target task.\n",
    "\n",
    " While effective, this method requires substantial labelled data, which can be costly and time-consuming\n",
    " to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Instruction Fine-Tuning via Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This method relies on providing the LLM with natural language instructions, useful for creating spe\n",
    "cialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the\n",
    " quality of the prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Pre-training vs Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:300px\">\n",
    "    <img src=\"image/table_pre_fine.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Importance of Fine-Tuning LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. *Transfer Learning*: Fine-tuning leverages the knowledge acquired during pre-training, adapting it to specific tasks with reduced computation time and resources.\n",
    "2. *Reduced Data Requirements*: Fine-tuning requires less labelled data, focusing on tailoring pre-trained features to the target task.\n",
    "3. *Improved Generalisation*: Fine-tuning enhances the model’s ability to generalise to specific tasks or domains, capturing general language features and customising them.\n",
    "4. *Efficient Model Deployment*: Fine-tuned models are more efficient for real-world applications, being computationally efficient and well-suited for specific tasks.\n",
    "5. *Adaptability to Various Tasks*: Fine-tuned LLMs can adapt to a broad range of tasks, performing well across various applications without task-specific architectures.\n",
    "6. *Domain-Specific Performance*: Fine-tuning allows models to excel in domain-specific tasks by adjusting to the nuances and vocabulary of the target domain.\n",
    "7. *Faster Convergence*: Fine-tuning usually achieves faster convergence, starting with weights that already capture general language features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Apopular method to utilise your own data is by incorporating it into the prompt when querying the LLM\n",
    " model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant\n",
    " data and using it as additional context for the LLM. Instead of depending solely on knowledge from the\n",
    " training data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data\n",
    " retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "With RAG architecture, organisations can deploy any LLM model and enhance it to return\n",
    " relevant results by providing a small amount of their own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This\n",
    " process avoids the costs and time associated with fine-tuning or pre-training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:400px\">\n",
    "    <img src=\"image/rag.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Traditional RAG Pipeline and Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. *Data Indexing*: Organise data efficiently for quick retrieval. This involves processing, chunking,\n",
    " and storing data in a vector database using indexing strategies like search indexing, vector indexing,\n",
    " and hybrid indexing\n",
    "2. *Input Query Processing*: Refine user queries to improve compatibility with indexed data. This\n",
    " can include simplification or vector transformation of queries for enhanced search efficiency.\n",
    "3. *Searching and Ranking*: Retrieve and rank data based on relevance using search algorithms\n",
    " such as TF-IDF, BM25, and deep learning models like BERT to interpret the query’s intent and\n",
    " context.\n",
    "4. *Prompt Augmentation*: Incorporate relevant information from the search results into the origi\n",
    "nal query to provide the LLM with additional context, enhancing response accuracy and relevance.\n",
    "5. *Response Generation*: Usetheaugmentedprompttogenerate responses that combine the LLM’s\n",
    " knowledge with current, specific data, ensuring high-quality, contextually grounded answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Benefits of Using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ *Up-to-Date and Accurate Responses*: Enhances the LLM’s responses with current external\n",
    " data, improving accuracy and relevance.\n",
    "+ *Reducing Inaccurate Responses*: Grounds the LLM’s output in relevant knowledge, reducing\n",
    " the risk of generating incorrect information.\n",
    "+ *Domain-Specific Responses*: Delivers contextually relevant responses tailored to an organisa\n",
    "tion’s proprietary data.\n",
    "+ *EfficiencyandCost-Effectiveness*: Offersacost-effective method for customising LLMs without\n",
    " extensive model fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "####  Challenges and Considerations in Serving RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. *User Experience*: Ensuring rapid response times suitable for real-time applications.\n",
    "2. *Cost Efficiency*: Managing the costs associated with serving millions of responses.\n",
    "3. *Accuracy*: Ensuring outputs are accurate to avoid misinformation.\n",
    "4. *Recency and Relevance*: Keeping responses and content current with the latest data.\n",
    "5. *Business Context Awareness*: Aligning LLM responses with specific business contexts.\n",
    "6. *Service Scalability*: Managing increased capacity while controlling costs.\n",
    "7. *Security and Governance*: Implementing protocols for data security, privacy, and governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for Choosing Between RAG and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When considering external data access, RAG is likely a superior option for applications needing to access\n",
    " external data sources. Fine-tuning, on the other hand, is more suitable if you require the model to ad\n",
    "just its behaviour, and writing style, or incorporate domain-specific knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " In terms of suppressing\n",
    " hallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to gen\n",
    "erating incorrect information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:450px\">\n",
    "    <img src=\"image/compare_rag_fine.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
