{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11:  Multimodal LLMs and their Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"background-color:white; padding:10px; display:flex; justify-content:center;height:450px\">\n",
    "    <img src=\"image/timeline-MMD.png\" alt=\"\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Vision Language Model (VLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Vision language models encompass multimodal models capable of learning from both images and text\n",
    " inputs. \n",
    " \n",
    " They belong to the category of generative models that utilise image and text data to produce\n",
    " textual outputs. \n",
    "\n",
    " Certain advanced vision language models\n",
    " can also understand spatial attributes within images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "###  Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Vision-language models adeptly integrate both visual and textual information, leveraging three fundamental components:\n",
    "\n",
    "+ Image Encoder: This component translates visual data (images) into a format that the model can process.\n",
    "+ Text Encoder: Similar to the image encoder, this component converts textual data (words and sentences) into a format the model can understand.\n",
    "+ Fusion Strategy: This component combines the information from both the image and text encoders, merging the two data types into a unified representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " These elements work collaboratively, with the model’s learning process (loss functions) specifically tai\n",
    "lored to the architecture and learning strategy employed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Constrative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Contrastive learning is a technique that focuses on understanding the differences between data points. It\n",
    " computes a similarity score between instances and aims to minimise contrastive loss, making it particu\n",
    "larly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation\n",
    " process to classify unseen data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**How it works**\n",
    "\n",
    "CLIP is a model that utilises contrastive learning to compute similarity between text and image embeddings through textual and visual encoders. It follows a three-step process for zero-shot predictions:\n",
    "\n",
    "+ Pre-training: Trains a text and image encoder to learn image-text pairs.\n",
    "+ Caption Conversion: Converts training dataset classes into captions.\n",
    "+ Zero-Shot Prediction: Estimates the best caption for a given input image based on learned similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Fine-tuning of multimodal models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LoRA and\n",
    " QLoRA can be utilised. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "LLM-Adapters integrate various adapter\n",
    " modules into the pre-trained model’s architecture, enabling parameter-efficient fine-tuning for diverse\n",
    " tasks by updating only the adapter parameters while keeping the base model parameters fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(IA)³,\n",
    " or Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learn\n",
    "ing vectors to weight model parameters through activation multiplications, supporting robust few-shot\n",
    " performance and task mixing without manual adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Dynamic adaptation techniques like DyLoRA allow for the training of low-rank adaptation blocks across different ranks, optimising\n",
    " the learning process by sorting the representations during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " LoRA-FA, a variant of LoRA, optimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a\n",
    " random projection while training the other, thereby reducing the number of parameters by half without\n",
    " compromising performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The Efficient Attention Skipping (EAS) module introduces a novel parameter and computation\n",
    "efficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and\n",
    " computation costs for downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "MemVP integrates visual prompts\n",
    " with the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time\n",
    " and inference latency, ultimately outperforming previous PEFT methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Full-parameter Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Methods such as those introduced by LOMO and MeZO provide alternative solutions by focusing\n",
    " on memory efficiency:\n",
    " + LOMO utilises a low-memory optimisation technique derived from Stochastic\n",
    " Gradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser.\n",
    " \n",
    " + MeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes\n",
    " to compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint\n",
    " equivalent to inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
